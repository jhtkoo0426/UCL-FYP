\documentclass[11pt, a4paper]{report}

% Page styling
\usepackage{extsizes}
\usepackage{latexsym}
\usepackage[a4paper,margin=3cm]{geometry}
\usepackage{changepage}

% Spacing
\usepackage{setspace}
\setstretch{1.05}

% Tables
\usepackage{booktabs}
\setlength{\columnsep}{1cm}

% Links
\usepackage{hyperref}

% Figures
\usepackage{float}
\usepackage{subfig}
\usepackage{caption}
\captionsetup[figure]{font=small,labelfont=Small}

% Equations% Equations & Theorems
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools}
\usepackage{graphicx,color}

% Citations
\usepackage{biblatex}
\addbibresource{citations.bib}



%----------------------------------------------------------------------------------
% Title Page
%----------------------------------------------------------------------------------
\title{{\vspace{-2in}\includegraphics[width=\textwidth]{docs/Project Report/Media/ucl_logo.png}}\\
\vspace{2cm}
\begin{adjustwidth}{1cm}{1cm}
    \centering
    \Huge Robust Robotic Grasping Utilising Touch Sensing
\end{adjustwidth}}
\\
\date{Submission date: \today}
\author{Koo Ho Tin Justin\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the BSc degree in Computer Science at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
\emph{Either:} The report may be freely copied and distributed provided the source is explicitly acknowledged
\newline
\emph{Or:}\newline
The report will be distributed to the internal and external examiners, but thereafter may not be copied or distributed except with permission from the author.}
\\ \\
BSc Computer Science\\ \\
Supervisors: Prof. Marc Deisenroth, Dr. Yasemin Bekiroglu}



\begin{document}
\onehalfspacing
\maketitle
\pagenumbering{roman}

%----------------------------------------------------------------------------------
% Abstract
%----------------------------------------------------------------------------------
\begin{abstract}
Robotic grasp synthesis has been studied extensively as robotic grasping skills have a significant impact on the success of subsequent manipulation tasks. Various approaches have been proposed for robotic grasp planning, with different assumptions regarding the available information about the type of objects in question (known, unknown, familiar). These approaches range from heuristic rules, and designing simplifying hand models, to complete end-to-end systems inferring grasp parameters from raw data.\\

However, most of these approaches do not address robustness in grasping, which refers to the ability of a robot to perform a grasping task consistently and accurately even in the case of unexpected disturbances or large degrees of errors in perception. There are several fundamental problems that need to be addressed to achieve better robustness in grasping tasks. These include mainly dealing with uncertainties in sensing, actuation and perceptual data.\\

In this project, we study how to build a robust learning framework that can be employed to construct robust grasp configurations using multi-modal data, e.g. tactile, and visual. The project addresses the following main issues with the robotic grasping systems: a) balancing the trade-off between data representation and data dimensionality; b) analyzing the modelling effects of different modalities, e.g. tactile and visual, and features to capture the underlying characteristics of the overall grasping process; c) a correction policy that relies on assessing grasp success before further manipulation using perceptual data, to choose the right grasping configuration.
\end{abstract}


%----------------------------------------------------------------------------------
% Acknowledgements
%----------------------------------------------------------------------------------
\renewcommand\abstractname{Acknowledgments}
\begin{abstract}
    
\end{abstract}


%----------------------------------------------------------------------------------
% Contents
%----------------------------------------------------------------------------------
\tableofcontents


%----------------------------------------------------------------------------------
% Chapter 1: Introduction
%----------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:1}
\pagenumbering{arabic}
\setcounter{page}{1}


\section{Outline of Problem}
\label{sec:1.1}
Humans are able to grasp objects seemingly intuitively. We can efficiently identify and reach for objects, adjust our finger placements and optimally balance contact forces by coordinating our wrists, arms, and shoulders when we approach and lift objects. Our sense of touch plays a major role, providing us with information about the object's size, shape, texture, weight, and other physical properties. This information is processed by our brains, giving us instructions on how to adjust our grasp pose and apply the appropriate amount of force needed to pick up the object. Without the sense of touch, grasping objects would be much more difficult and less efficient. Therefore, incorporating touch sensing into robotic grasping can help robots better identify and adjust to objects, making them more effective in interacting with the physical world.\\

Recent advancements in machine learning, computer vision, and robotics have led to remarkable progress in the field of robotic grasping. This technology holds great promise for a range of applications, including manufacturing, warehouse automation, and household robotics. However, little work has been done on increasing robustness in grasping systems that utilise multi-modal (tactile and visual) data.\\

Robustness in a robotic grasping system refers to its versatility of accurately recognising and grasping objects with varying geometric features (width, depth, curvature, etc) and its ability to successfully grasp the objects. Robustness is increasingly becoming a vital factor in the study of grasp stability and quality, however, incorporating robustness makes it practically difficult to design a single gripper that can effectively grasp all objects. As a result, researchers and engineers have developed dedicated grippers tailored to specific grasping tasks or types of objects.\\

However, developing a robust and efficient grasping system presents several challenges. One major hurdle is the incorporation of touch sensing into robotic grasping, which is hindered by hardware limitations such as sensor sensitivity and cost, as well as difficulties in integrating tactile inputs into standard control schemes \cite{Calandra_2018}. Consequently, most robotic grasping research has focused on vision and depth as the primary input modalities \cite{Calandra_2018}. However, vision-based grasping approaches have limitations in measuring and reacting to ongoing contact forces, which hinders the full potential benefits of interaction. As a result, these approaches mostly rely on pre-selecting a grasp configuration, such as location, orientation, and forces, before making contact with the object. Addressing these limitations and integrating tactile feedback could significantly improve robotic grasping, leading to more efficient and effective interactions between robots and the physical world.\\

Another challenge is developing a concrete strategy for improving grasp quality and planning while conserving the efficiency of the overall system. Results from our \hyperref[chap:2]{literature survey} suggest that such strategies can be summarised into three types: (1) choice of object representation, where coarse approximations of the underlying true shape of an object can simplify the generation of new grasps \cite{de_Farias_2021} \cite{geidenstam_2009}; (2) use of local symmetry properties to capture key geometric features of the object to generate promising grasp candidates \cite{de_Farias_2021}; (3) optimising shape modelling where target objects are parameterized using smooth, differentiable functions from point clouds using spectral analysis \cite{de_Farias_2021}.


\section{Project Aims}
\label{sec:1.2}
Having gained a comprehensive understanding of the previously discussed optimizations and strategies, our research project focuses on the development and testing of a robust robotic system that can learn to pick up an object with simple geometry using a two-finger hand. The project will take a learning-based approach to grasping through, for example, Bayesian optimization \cite{nogueira2016unscented, frazier2018tutorial}. The learning-based approach should be compared with a baseline approach from the related literature (e.g. \cite{nogueira2016unscented, danielczuk2020exploratory, breyer2020volumetric}) for evaluation, for example, a simple regression classifier to differentiate between good and bad grasps.


\newpage
\section{Project Objectives}
\label{sec:1.3}
This project is concerned with the development of a complete simulation with supporting features for data collection and analysis. Therefore, the aims of this project are split according to our system requirements, which we describe in detail below. These objectives are expected to be completed throughout the academic year:
\begin{enumerate}
    \item Set up a simulation environment, e.g. PyBullet \cite{coumans2021} or NVIDIA Isaac. 
    \item Create a data collection pipeline for sensory data (e.g. visual and force/torque readings) via the simulator.
    \item Apply basic simulation functionalities: position control and vision sensing on the robot.
    \item Implement and test baseline (e.g. \cite{breyer2020volumetric}, and a basic approach such as executing predefined grasps per object model given object pose).
    \item Build the learning framework:
    \begin{enumerate}
        \item Learning grasps based on Bayesian Optimization, from a chosen scene representations such as signed distance function 
        \item Picking with two fingers given object model (primitive shapes such as box, sphere, cylinder) and pose, all learned by trial and error
    \end{enumerate}
\end{enumerate}


\section{Project Approach}
\label{sec:1.4}
This section discusses the important phases of the project and the approach taken at each phase. Technologies required for completing a particular phase will be documented and justified.


\subsection{Planning Phase - Design and Research}
\label{sec:1.4.1}
We first conduct a literature survey to explore and categorise research projects related to robotic grasping by data input, grasping method and the problem these projects address. Simultaneously, background research was conducted on grasp learning \cite{platt2022grasp} and robot manipulation to obtain a general understanding of the project's task.


\subsection{Early Development Phase - Pybullet Simulation}
\label{sec:1.4.2}
This project is completely conducted via a simulation environment, including data collection, robot manipulation, experimentation and testing of our approaches. Therefore, the development of a Pybullet simulation was considered a top priority in the early phases of the project. The robot we used for our simulation consists of the following components:
\begin{enumerate}
    \item Arm: UR5 Robot Arm Manipulator
    \item Gripper: Robotiq 2-Finger Adaptive Robot Gripper 85
    \item Tactile sensors: DIGIT tactile sensors \cite{Lambeta2020DIGIT} mounted onto each finger of the gripper
\end{enumerate}
The Pybullet simulation should support the following features for our project:
\begin{enumerate}
    \item manual robot (end effector) manipulation
    \item real-time display of tactile data (depth and colour cameras)
    \item a grasp planner using inverse kinematics
    \item a random end effector pose generator for creating varying grasp poses
    \item a tactile data collection pipeline using randomly-generated end effector grasp poses
\end{enumerate}
\noindent This will be further discussed in \hyperref[sec:3.1]{Section 3.1}.


\subsection{Baseline Development Phase}
\label{sec:1.4.3}
This phase focuses on determining the optimal dataset representation for our collected multi-modal data, consisting of tactile sensor readings, visual end effector poses relative to the arm, and the respective grasp outcomes of each end effector pose. In addition, we experiment with several feature engineering techniques, for example, dimensionality reduction using ConvNets and Principal Component Analysis (PCA).


\subsection{Execution Phase - Learning Approach Implementation}
\label{sec:1.4.4}
Following the outcomes of our baseline approach where we identified the optimal feature representation of our dataset, we move on to develop our learning-based approach which is trained on three primitive object types. In addition to tactile sensor readings and visual data (randomly-generated end effector poses), we include the object features for each object class in our dataset, and train this dataset on a multilayer perceptron (MLP). Finally, we conduct several interesting experiments, including (1) training our MLP using various segmentations of our dataset; and (2) validating our MLP on unseen objects that comprise similar geometric features to that of the primitive objects that our MLP was trained on.


\subsection{Documentation Phase}
\label{sec:1.4.5}
The final phase of the project involves the completion of the project report, writing documentation on setting up and using the simulation, as well as including a brief description of the methodology behind our baseline and learning-based approaches which are implemented in Jupyter Notebook.


\section{Report Structure}
\label{sec:1.5}
Chapter 2 gives a brief overview of the background of robotic grasping and relevant literature on the project. We investigate common grasp representations and grasp quality evaluation techniques, and analyse various data representations of multi-modal data (tactile, sensory, temporal) and policy-learning approaches.\\

Chapter 3 documents the system architecture for the project and the development of a Pybullet simulation, including input components to enable data collection and simulation of our robot setup.\\

Chapter 4 documents the development and testing of our baseline approach using a binary logistic regression classifier.\\

Chapter 5 documents the development and testing of our proposed approach using a multilayer perceptron (MLP) model that attempts to infer good grasp configurations on three primitive object classes from random hand poses, trained on data collected from the Pybullet simulation. The dataset includes tactile sensor readings, random hand poses and geometric features of the target objects.\\

Chapter 6 summarises the results and performances of the project, as well as makes suggestions for future work and improvement based on these findings.



%----------------------------------------------------------------------------------
% Section 2: Background and Literature Review
%----------------------------------------------------------------------------------
\chapter{Background and Literature Review}
\label{chap:2}


\section{Robotic Grasping}
\label{sec:2.1}
In the field of robotics, grasping is a fundamental yet challenging skill of robots which refers to the autonomous ability of a robot to grasp and move objects with its mechanical grippers or other end effectors \cite{zhang2022robotic}. It demands precise coordination between the visual perception of the surrounding environment and efficient grasp planning, as well as a robot's adaptiveness to unseen object classes (robustness).\\

Robotic grasping is far away from developing into an intuitive and instinctive sense as in human beings. There are many unknown factors that enable humans to flawlessly perform grasps when provided with visual and tactile information. However, with the advancements of deep learning and computer vision, semantic grasping is becoming the main focus and basis for autonomous robotic grasping systems \cite{zhang2022robotic}, where a robot is trained on monocular images of a user-specified object type \cite{jang2017endtoend}.\\

To successfully grasp an object, a robot needs to detect the object's location (through, for example, a wrist camera), orientation, and size accurately, estimate the best grasp point and orientation, and then execute the grasp with appropriate force and control to ensure a secure hold. The grasping process can be performed using various types of end effectors, including grippers, suction cups, or specialized tools designed for specific tasks.\\

Therefore, accurate and efficient grasp representation and planning are paramount to ensure a smooth grasping process when performing grasping tasks.


\newpage
\subsection{Grasp Representation}
\label{sec:2.1.1}
A grasp in robotic grasping is a specific configuration of a robotic hand or gripper which allows it to firmly hold an object. These representations can be used in different grasp planning algorithms and can be combined to create more complex representations of grasps. Commonly-used grasp representations include: 


\subsubsection{Meshes}
\label{sec:2.1.1.1}
In computer graphics and object modelling, a (polygon) mesh is a 3-dimensional representation or rigid reconstruction of the surface of an object. Typically, the representation is a collection of vertices, edges and faces that define the object's shape. Faces usually consist of triangles an object's surface that is composed of vertices, edges, and faces. Grasps can be represented as a set of vertices on the mesh that corresponds to the location of the gripper's fingertips and palm.


\subsubsection{Point Clouds}
\label{sec:2.1.1.2}
In the context of robotics, a point cloud is a discrete set of 3-dimensional Cartesian coordinates which collectively represents the surface of an object. The location of a gripper's fingertips can be represented as sets of points on the point cloud in order to annotate a grasp.\\

In addition to grasp representations, point clouds can be merged for surface reconstruction (using, for example, Poisson surface reconstruction), which creates a mesh (or set of surfaces) that approximate the true shape of the target object.\\

In the Pybullet simulation for this project, we include a simple feature that generates a point cloud of the rendered object of the simulation which can be inspected visually. In the physical world, a point cloud is generated using 3D scanners or photogrammetry software, which can be rendered and inspected in a virtual world.


\subsubsection{Hand Configurations}
\label{sec:2.1.1.3}
A hand (or end effector) configuration describes the joint angles or positions of a robotic hand or gripper relative to the robot itself. This is a simple and straightforward approach to representing grasps.\\

In this project, we use the terms "hand configurations" and "end effector poses" interchangeably.


\section{Tactile Sensing for Robotic Grasping}
\label{sec:2.2}
Tactile sensing has been widely used to provide robots with information about target objects, allowing them to grasp and manipulate said objects with greater precision.

\begin{enumerate}
    \item detect object properties
    \item monitor grasp stability (Yasemin's thesis)
    \item provide feedback during manipulation, detect object contact
\end{enumerate}


\section{Common Grasping Approaches}
\label{sec:2.3}
Humans rely on rich tactile feedback to grasp objects, however, most of the recent robotic grasping studies only focus on visual input. The authors propose an end-to-end action-conditional model (a multimodal CNN) to learn regrasping policies from visual-tactile data, by predicting the outcome of a candidate grasp adjustment, then executing a grasp by iteratively selecting the most promising actions\cite{Calandra_2018}.


\section{Conclusion}
\label{sec:2.4}
% Conclude that not much work has been done on using tactile + visual data for robust grasping systems


%----------------------------------------------------------------------------------
% Section 3: Preliminaries
% Describes the detailed process of developing the simulation for the project
%----------------------------------------------------------------------------------
\chapter{Preliminaries}
\label{chap:3}


\section{Pybullet overview & project simulation functionalities}
Arm control, grasp control, position control, gripper control, get 6d pose, tactile data visualisation

\subsection{Parameters}
yaml parameters, loading parameters


\section{Describe data (depth colour, ee pose), how to extract (via camera)}


\subsection{Tactile data}
- visualise some tactile data (depth, colour pairs)


\subsection{Visual data}
- "Visual" as in relative of hand to object
- Each ee pose is a 6-tuple consisting of 3 Cartesian coordinates representing the spatial position of the end effector relative to the robot, as well as 3 Euler angles representing the orientation of the end effector

\subsection{}
\subsection{TACTO Camera for Live Tactile Data Visualisation}
\label{sec:}
- TACTO camera \cite{Wang2022TACTO}
- self.digits.render()



%----------------------------------------------------------------------------------
% Section 4: Baseline and Feature Representation Analysis
%----------------------------------------------------------------------------------
\chapter{Baseline and Feature Representation Analysis}
\label{chap:4}
This chapter proposes a binary logistic regression model (commonly known as binary logit) for the classification of grasps. The fundamental assumption of this approach is that grasps can be dichotomously categorized as either successful or unsuccessful outcomes. In addition, we seek an optimal feature representation for the tactile and/or visual data generated from randomly-synthesised grasps in our simulation.


\section{Intuition of the Logit Model}
\label{sec:4.1}
Assume that we have selected some representation of tactile and/or visual features for each grasp, denoted $x_i$. Our logit model attempts to classify the grasp with a binary label $y_i$. To achieve this, the weights $w$ of the model are fitted through a maximum likelihood estimation where the distribution of the features $x_i$. Assuming the grasp outcomes as a Bernoulli random variable, we can construct and minimise the following cross-entropy loss function and treat it as an optimisation problem:
\begin{equation}
    ln(L(w))=\sum^n_{i=1}y_i\cdot ln(\rho_y(y_i=1|x_i))+(1-y_i)\cdot ln(\rho_y(y_i=0|x_i))
\end{equation}
where $\rho_y(y_i=1|x_i)$ and $\rho_y(y_0=1|x_i)$ represent the probability of a grasp $x_i$ being successful $y_i=1$ or unsuccessful $y_i=0$ respectively.\\

Therefore using our learned logit model (with an optimised weights vector), we can determine whether the outcome (successful or unsuccessful) of an unseen grasp is ascertainable based on its feature representation.


\section{Dataset Generation}
\label{sec:4.2}
To generate a dataset for training our proposed logit model, we collect tactile and visual data by randomly generating grasps on a smooth rectangular box of dimensions (depth: 0.025, width: 0.05, height: 0.05) in our Pybullet simulation.


\subsection{Random Grasp Synthesis}
\label{sec:4.2.1}
To generate random grasps on the box, we first manipulate the robot's end effector manually into diverse positions within the box's vicinity and collect three distinct end effector poses $S_i:i=1,2,3$ to ensure some variability in grasping orientation, position and efficacy. The selected end effector poses are subsequently referred to as seed poses, denoted as a 6-tuple $S_i$:
\begin{equation}
    S_i=(x,y,z,r_x,r_y,r_z)
\end{equation}
For each seed pose, we generate a fixed number $n$ of random poses from the seed by adding a small amount of Gaussian noise $K_n$ with zero mean and unit variance to each dimension of $S_i$. We denote the generated grasp poses $X_v$ as follows:
\begin{equation}
    X_v=\Big\{S_i+K_i:K_i\sim\mathcal{N}(0,0.01),\;i=1,2,3,...,n\Big\}
\end{equation}
The variance of the Gaussian noise can be adjusted for a more/less dispersed distribution of the random grasp poses. It is essential to balance the trade-off between generating diverse grasp poses (and their corresponding data) and ensuring that the poses are likely to succeed. Notably, the impact of the variance of the Gaussian noise will be contingent on the characteristics of the object being grasped, which may necessitate modifying the variance according to the object's properties. This will be discussed in Section 5 where we introduce geometric features of objects into our classifier approach.
 

\newpage
\subsection{Data Collection Pipeline}
\label{sec:4.2.2}
Prior to executing each randomly generated pose $X_{v,i}$ as a pick-and-place task, we first reset the position and orientation of our robot to an initial, predefined configuration (\ref{fig:4.1a}). Next, we utilise Pybullet's built-in inverse kinematics method to manipulate the robot's end effector to $X_{v,i}$. To avoid collision with the object, we include a vertical padding distance $p_z$ above the object (\ref{fig:4.1b}). Then, the robot lowers the end effector by removing the padding $p_z$ to reach the target object and closes its gripper (\ref{fig:4.1c}).

\begin{figure}[H]%
    \centering
    \subfloat[\centering Reset robot to initial configuration]{{\includegraphics[width=0.47\textwidth]{docs/Project Report/Media/4_2_2_init_config.png}\label{fig:4.1a}}}%
    \qquad
    \subfloat[\centering Move end effector to specific pose (with vertical padding)]{{\includegraphics[width=0.47\textwidth]{docs/Project Report/Media/4_2_2_ee_pose_with_pad.png}\label{fig:4.1b}}}%
    \qquad
    \subfloat[\centering Remove vertical padding and close gripper]{{\includegraphics[width=0.47\textwidth]{docs/Project Report/Media/4_2_2_ee_pose_no_pad.png}\label{fig:4.1c}}}%
    \qquad
    \subfloat[\centering Lift object]{{\includegraphics[width=0.47\textwidth]{docs/Project Report/Media/4_2_2_pick_obj.png}\label{fig:4.1d}}}%
    \caption{Tactile and visual data collection procedure on a single generated hand pose}%
    \label{fig:4.1}%
\end{figure}

Before lifting the object, the robot measures the number of contact points between the two fingers and the object to ensure the grasp is stable. We define a grasp to be stable if there are a non-zero number of contact points, in which the robot records depth and colour tactile information using the DIGIT \cite{Lambeta2020DIGIT} tactile sensors on each finger. The depth read We concatenate each pair of depth and colour tactile readings into a $(160\times240)$ grey-scale image and a $(160\times240\times3)$ RGB image respectively.\\

After collecting the tactile readings, the robot performs a sanity check on the recorded depth tactile data. Since the colour data is an RGB representation of the depth data, it does not require any additional checks. To facilitate the sanity check, we analyze whether the average pixel value exceeds a particular threshold. This process enables us to remove any erroneous signals (\ref{fig:4.2a}) and preserve the precise depth data (\ref{fig:4.2b}). By utilizing this method, we can enhance the quality of the data and ensure the accurate lifting of the object.

\begin{figure}[H]%
    \centering
    \subfloat[\centering Invalid depth data]{{\includegraphics[width=7cm]{docs/Project Report/Media/4_2_2_invalid_tactile_data.png}\label{fig:4.2a}}}%
    \qquad
    \subfloat[\centering Valid depth data]{{\includegraphics[width=7cm]{docs/Project Report/Media/4_2_2_valid_tactile_data.png}\label{fig:4.2b}}}%
    \caption{Concatenated depth data for each finger}%
    \label{fig:4.2}%
\end{figure}

Once the robot verifies that the object is stably held (i.e. the tactile data is valid), it lifts the object vertically upwards by an offset of $p_z$ and holds the object for 750 steps (\ref{fig:4.1d}). Finally, the robot records a binary outcome of the grasp by determining if the change in the object's z-position $\Delta z$ is greater or equal to the vertical padding $p_z$:

\begin{equation}
    outcome=\begin{cases}1 & \Delta z\geq p_z \\ 0 & \Delta z<p_z\end{cases}
\end{equation}

After executing each random pose, we append the collected tactile data, end effector pose and grasp outcome to individual arrays. Once the required data for our $n$ random end effector poses has been collected, these arrays are saved to separate files which can be loaded for further analysis. Specifically, we create one .npy file each for the depth, colour, grasp outcomes, and random pose data.


\newpage
\section{Visualising the Dataset}
\label{sec:4.3}
In this section, we explore our data to understand any underlying patterns or characteristics and prevent the generation of abnormal grasp data (as in \ref{fig:4.2}).


\subsection{Visualising Tactile Data}
\label{sec:4.3.1}

\begin{figure}[H]%
    \centering
    \subfloat[\centering Successful grasp data]{{\includegraphics[width=\textwidth]{docs/Project Report/Media/4_3_successful_grasp_data.png} }}%
    \qquad
    \subfloat[\centering Unsuccessful grasp data]{{\includegraphics[width=\textwidth]{docs/Project Report/Media/4_3_unsuccessful_grasp_data.png} }}%
    \caption{Left to right by column: Depth readings, Colour (RGB) readings, Skeleton 2-finger model to represent a hand pose}
    \label{fig:4.3}%
\end{figure}


\subsection{Visualising Hand Poses}
\label{sec:4.3.2}
We create a visualisation of each hand pose $X_n$ in a 3-dimensional space, using its Cartesian coordinates and the "pitch-row-yaw" rotational matrix representation $R$ of its Euler angles ($\alpha,\beta,\gamma$) \cite{weisstein}:

\begin{equation}
    R=r_x\cdot(r_y\cdot r_z)
\end{equation}

\noindent where $r_x$, $r_y$ and $r_z$ are the rotational matrices about the $x$, $y$ and $z$-axis respectively:

\begin{figure}[H]
    \centering
    \begin{equation*}
        r_x = \begin{pmatrix}1&0&0 \\ 0&cos(\alpha)&sin(\alpha) \\ 0&-sin(\alpha)&cos(\alpha)\end{pmatrix}\;\;
        r_y = \begin{pmatrix}cos(\beta)&0&-sin(\beta) \\ 0&1&0 \\ sin(\beta)&0&cos(\beta)\end{pmatrix}\;\;
        r_z = \begin{pmatrix}cos(\gamma)&sin(\gamma)&0 \\ -sin(\gamma)&cos(\gamma)&0 \\ 0&0&1 \end{pmatrix}
    \end{equation*}
    \caption{Rotational matrices for each axis in the rotational matrix representation of Euler angles}
    \label{fig:4.3}
\end{figure}

\noindent Using this information, we annotate all randomly-generated hand poses from our dataset in 3 dimensions. Each 6D hand pose is represented using a skeleton 2-finger model in a similar manner to \cite{Bekiroglu2012LearningTA}, where green and red poses denote successful and unsuccessful grasps respectively:

\begin{figure}[H]%
    \centering
    \subfloat[\centering Box without poses]{{\includegraphics[width=0.29\textwidth]{docs/Project Report/Media/4_3_2_box_3d_plot.png} }}%
    \qquad
    \subfloat[\centering Orientation 1]{{\includegraphics[width=0.29\textwidth]{docs/Project Report/Media/4_3_2_all_ee_pose_visualise.png} }}%
    \qquad
    \subfloat[\centering Orientation 2]{{\includegraphics[width=0.29\textwidth]{docs/Project Report/Media/4_3_2_all_ee_pose_visualise2.png} }}%
    \caption{Visualisation of 200 random end effector poses}%
    \label{fig:4.4}%
\end{figure}


\section{Feature Engineering \& Data Pre-processing}
\label{sec:4.4}
We aim to assess the efficacy of utilising our multi-modal dataset for our grasping task through experimenting with various combinations of the tactile and visual (random end effector poses) data.


\subsection{Multi-modal Data Combinations}
\label{sec:4.4.1}
In this project, we segregate our dataset into tactile-only, visual-only and concatenated tactile and visual datasets:
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{ll p{10cm}}
            \toprule
            Data Combination & Dimensions & Creation \\
            \midrule
            Tactile only & $N\times160\times240\times4$ & Concatenating the depth ($N\times160\times240$) and color ($N\times160\times240\times3$) data \\
            Visual only & $N\times6$ & Random 6D end effector poses \\
            Tactile + Visual & $N\times(160\times240\times4+6)$ & Concatenating the flattened tactile-only and visual-only datasets \\
            \bottomrule
        \end{tabular}
    }
    \caption{Dataset combinations}
    \label{tab:4.1}
\end{table}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{docs/Project Report/Media/tactile_dataset_creation.png}
    \caption{Creating the tactile dataset}
    \label{fig:my_label}
\end{figure}


\subsection{Dimensionality Reduction}
\label{sec:4.4.2}
In this project, we represent tactile sensory readings as images, specifically, grayscale for depth data and RGB for colour data. As the dimensionality of this data has a significant impact on computational costs, we have employed dimensionality reduction techniques to alleviate this issue, including principal component analysis (PCA) and convolutional neural networks (CNN) for feature extraction.


\subsubsection{Principal Component Analysis (PCA)}
\label{sec:4.4.2.1}
{\color{red} [CHATGPT was used here for better phrasing - change later]}\\
PCA is a widely used linear technique that projects the high-dimensional input data onto a lower-dimensional space, capturing the maximum amount of variance in the data with fewer dimensions. By reducing the dimensionality of the data, PCA can facilitate more efficient computation and enable the extraction of meaningful patterns and features from the data.\\

PCA is particularly useful for concatenated tactile and visual data, as it can extract relevant features from both modalities and combine them to form a lower-dimensional representation of the data. For example, PCA can be used to extract principal components that represent the most significant tactile and visual features, which can then be combined to form a lower-dimensional representation of the data. The resulting representation can be used as input to machine learning algorithms for classification or regression tasks.\\

It is worth noting that the effectiveness of PCA for dimensionality reduction depends on several factors, such as the number of input dimensions, the amount of variance in the data, and the distribution of the data. Additionally, the choice of the number of principal components to retain can also impact the performance of the method. Hence, a careful evaluation of the effectiveness of PCA for the given task is necessary, including the selection of an appropriate number of principal components to retain, and any trade-offs between computation time and performance.


\subsubsection{Convolution Neural Network (CNN) Feature Extraction}
\label{sec:4.4.2.2}
{\color{red} [CHATGPT was used here for better phrasing - change later]}\\
In the context of multi-modal sensor data, feature extraction using convolutional neural networks (CNNs) has gained popularity due to their ability to capture spatial and temporal patterns in the data. The CNN architecture, as described with interleaved convolution and pooling layers, is well-suited for detecting local features in the input data. The convolution layers extract low-level features, such as edges or textures, while the pooling layers help reduce the dimensionality of the feature maps and provide translational invariance to the learned features. The combination of these layers can create a hierarchy of feature representations, with higher-level features capturing more complex and abstract patterns in the data.

\begin{figure}[H]
    \includegraphics[scale=0.7]{docs/Project Report/Media/nn.png}
    \caption{Architecture for feature extraction CNN}
    \label{fig:4.5}
\end{figure}

\ref{fig:4.5} visualises the specific CNN architecture used in the baseline, consisting of three convolutional layers interleaved with MaxPool2D layers with a 2 × 2 kernel, which is effective in capturing features from concatenated tactile and visual sensory data. 


\subsection{Data Pre-processing}
\label{sec:4.4.3}
{\color{red} [CHATGPT was used here for better phrasing - change later]}\\
Normalization of data prior to model training is a well-established practice in machine learning, known to improve the accuracy and efficiency of the learning process. In the context of tactile sensor readings and end effector poses, normalization is particularly relevant due to the large differences in the scale of the data. For instance, RGB pixel values for the colour dataset vary between 0 to 255, while Euler angles in the hand pose dataset vary between $-\pi$ and $\pi$.\\

Failure to normalize the data can result in numerical instability, as differences in scale can cause the weights of the model to become disproportionately small or large, thereby hampering effective learning. Moreover, normalization can improve model generalization by reducing the impact of outliers and mitigating the effects of varying ranges of feature values in the data. Finally, normalization facilitates comparison of the relative importance of different features, thereby enabling the identification of the most significant ones and elimination of redundant or less important ones.


\section{Logit Model Training, Evaluation and Model Selection}
\label{sec:4.5}
Based on our findings (\hyperref[sec:4.3]{Section 4.3}), we propose a simple binary logistic regression (LR) classifier to predict the grasp outcome for a given random end effector pose. Ideally, this LR model is able to determine a decision boundary for segregating successful and unsuccessful grasp poses.\\

We train the classifier using the three discussed dataset combinations (\hyperref[sec:4.4.1]{Section 4.4.1}). Additionally, we apply a combination of dimensionality reduction techniques to these datasets discussed in (\hyperref[sec:4.4.2]{Section 4.4.2}), including ConvNet only and ConvNet with PCA (top $k$ principal components).\\

\hyperref[fig:4.8]{Figure 4.8} visualises the change in the amount of variance captured by $k$ components. We select $k=5$ components which captures $79.7\%$ variance of the dataset.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{docs/Project Report/Media/4_5_k_against_variance.png}
    \caption{Change in variance captured by $k$ components}
    \label{fig:4.8}
\end{figure}


\subsection{Summary of training results}
\label{sec:4.5.1}
Table 4.2 shows the accuracy for each LR model trained based on the dataset type and preprocessing method
\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lccccc}
        \toprule
        & \multicolumn{3}{c}{Dataset} & \multicolumn{2}{c}{Property of sample} \\
        \cmidrule{2-4}\cmidrule{5-6}
        Preprocessing & Visual only & Tactile only & Both & Shape & Dimensionality \\
        \midrule
        Raw & 51.25\% & 85.00\% & 85.00\% & 160x240x4 & 153600 \\
        CNN & -- & 83.33\% & 81.25\% & 64x10x15  & 9600 \\
        CNN + PCA ($k=5$) & -- & 81.25\% & 79.17\% & 5 & 5\\
        \bottomrule
    \end{tabular}
    \caption{Accuracy of LR model on dataset variations}
    \label{tbl:4.2}
\end{table}


\subsection{PCA vs CNN}
\label{sec:4.5.2}
{\color{red} [CHATGPT was used here for better phrasing - change later]}\\
Compared to traditional techniques like PCA, CNNs can learn complex, non-linear relationships between the input features and the target variable. This is particularly useful when dealing with high-dimensional data, where PCA may not capture the underlying patterns in the data. Moreover, the ability of CNNs to learn hierarchical feature representations can make them more interpretable, as each layer captures increasingly complex and abstract features, leading to a better understanding of the underlying mechanisms of the problem.


\subsection{Balancing trade-off between accuracy and dimensionality}
\label{sec:4.5.2}
Based on the results presented in \hyperref[tbl:4.2]{Table 4.2}, we have determined that using both tactile and visual data together produces the highest accuracy for our LR models. While raw data performed better than other pre-processing techniques, we recognize that the dimensionality of the data must also be considered. Therefore, we have found that extracting features from our dataset using a ConvNet and selecting only the top $k=5$ components provides comparable accuracy with significantly lower dimensionality.


\section{Conclusion}
\label{sec:4.6}


\subsection{Feature Representation}
\label{sec:4.6.1}
We use tactile + visual data, with CNN for feature extraction and PCA for further dimensionality reduction to $N\times 5$ dimensions to represent the sensory data.

[Evaluate more]



%----------------------------------------------------------------------------------
% Section 5: Proposed Approach
%----------------------------------------------------------------------------------
\chapter{Multilayer Perceptron Artificial Neural Network for Grasp Stability Prediction}
\label{chap:5}

\section{Mutlilayer Perceptrons and Motivation}
\label{sec:5.1}
A multilayer perceptron (MLP) as a feedforward neural network consisting of a minimum of three fully-connected layers of perceptrons (neurons). Every MLP contains an input layer that receives input data, an output layer that produces a prediction based on patterns within the data, and at least one hidden layer that enable the MLP to learn complex representations of the data.\\

For robotic grasping on a specific object, there may exist certain grasp configurations that yield a high grasp stability and success rate. For instance, grasping an object around its center of gravity can minimise rotational forces on the object that could cause it to tilt, hence increasing the probability of a successful grasp. MLPs excel at learning non-linear relationships between input and output data. Therefore, intuitively, a MLP is a good candidate to learn such patterned configurations from the complex dataset and generalise them into a simple grasping strategy based on different object types.\\

To introduce robustness in robotic grasping for various object types, we trained our MLP model on three primitive object categories, namely rectangular boxes, cylinders, and bottle-shaped objects (which are more rigid than typical cylinders). Our intention is to explore the potential influence of an object's geometric characteristics, such as its width and height, on the model's accuracy.


\section{Dataset Collection and Representation}
\label{sec:5.2}


\subsection{Object Variations}
\label{sec:5.2.1}
To enhance the robustness of our model, we have trained it on three primitive object categories that collectively represent a sizable proportion of daily-life objects: rectangular boxes, cylinders and bottles. For each object category, we generate three instances with slightly different dimensions. The 3D meshes of these object variations and their dimensions are listed in \hyperref[tbl:5.1]{Table 5.1}.

\begin{table}[H]
    \centering
    \caption{3D meshes of all experimental object variations}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccccc}
            \toprule
            & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/block1.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/block2.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/block3.png} &
            \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/cylinder1.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/cylinder2.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/cylinder3.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/mustard_bottle1.png} & \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/mustard_bottle2.png} &  \includegraphics[width=0.1\textwidth]{docs/Project Report/Media/mustard_bottle3.png} \\
            & Box 1 & Box 2 & Box 3 & Cylinder 1 & Cylinder 2 & Cylinder 3 & Bottle 1 & Bottle 2 & Bottle 3 \\
            \midrule
            Depth  & 0.025 & 0.03  & 0.05  & 0.04 & 0.045  & 0.05  & 0.06 & 0.04 & 0.04 \\
            Width  & 0.05  & 0.025 & 0.025 & 0.04 & 0.045  & 0.05  & 0.04 & 0.06 & 0.06 \\
            Height & 0.05  & 0.045 & 0.04  & 0.05 & 0.035  & 0.045 & 0.04 & 0.06 & 0.04 \\
            Radius & --    & --    & --    & 0.02 & 0.0225 & 0.025 & --   & --   & --   \\
            \bottomrule
        \end{tabular}
    }
    \label{tbl:5.1}
\end{table}


\subsection{Data Collection}
\label{sec:5.2.2}
Utilising our Pybullet data collection pipeline from \hyperref[sec:4.2]{Section 4.2}, we collect multi-modal data for every object variation. Then, for each variation, we manually select $i=4$ seed poses and record the data of 20 successful and 20 unsuccessful grasps with Gaussian noise applied to the seed pose. This forms a dataset of 9x160=1440 grasps in total (720 stable and 720 unstable grasps) consisting tactile sensor readings and hand poses.


\subsection{Geometric Features of an Object}
\label{sec:5.2.3}
Each primitive object category in our dataset possesses unique characteristics. For instance, the curvature of an object's surface can differentiate cylinders and bottles from boxes with ease. As objects within a primitive class often share similar geometric features (such as the curvature of cylinders), our model could potentially generalize successful and unsuccessful grasp poses within each object category by establishing a connection between those poses and the object's properties.\\

To this end, we extracted and analysed several geometric features of the 9 object variations in order to determine which of them can effectively differentiate between the object categories.


\subsubsection{Principal Curvature}
\label{sec:5.2.3.1}
The principal curvatures of an object mesh is pair of maximum and minimum values of curvatures at each vertex of the mesh, expressed by the eigenvalues of the shape operator at that vertex \cite{enwiki:1141654906}. We investigate whether the top $k$ principal curvatures are able to categorize the object variations correctly.

\vspace*{-0.7cm}
\begin{figure}[H]%
    \centering
    \subfloat[\centering Top $k=2$ components; Variance captured: 91.43\%]{{\includegraphics[width=0.4\textwidth]{docs/Project Report/Media/principal_curvature_2d.png} }}%
    \qquad
    \subfloat[\centering Top $k=3$ components; Variance captured: 97.34\%]{{\includegraphics[width=0.4\textwidth]{docs/Project Report/Media/principal_curvature_3d.png} }}%
    \caption{Clustering using PCA-extracted components based on principal curvatures of objects}%
    \label{fig:5.1}%
\end{figure}

\hyperref[fig:5.1]{Figure 5.1} visualises the top $k=2$ and $k=3$ principal curvatures for all the object variations in our dataset. We found that the principal curvatures were able to:
\begin{enumerate}
    \item successfully cluster the object variations for each category
    \item capture a high amount of variance of the curvatures for all objects
\end{enumerate}
\noindent Thus, we conclude that principal curvatures is a valid representation of geometric features for our dataset of objects.


\section{Preprocessing}
\label{sec:5.3}
Based on our analysis results of our baseline approach in \hyperref[sec:4.7]{Section 4.7}, we concluded that under limited dimensionality circumstances, the Logistic Regression model fits the dataset with the highest accuracy using a convolutional neural network as a dimensionality reduction technique. Therefore, before training our MLP, we extract important features from our dataset using the same \hyperref[fig:4.5]{CNN} architecture. This results in a condensed dataset of shape $1200\times512$.\\

To improve the convergence speed and performance for training our MLP model, we normalise the tactile and visual datasets with zero mean and unit variance. This also prevents the initialisation of large weights during back-propagation.


\section{Training}
\label{sec:5.4}
We trained our MLP model on two versions of our dataset:
\begin{enumerate}
    \item tactile and visual data (end effector poses only)
    \item tactile and visual data (end effector poses and object geometric features)
\end{enumerate}


\subsection{Training details}
Split dataset into 80\% training and 20\% validation;


\subsection{Loss function}
simple binary cross-entropy with logits loss (sigmoid + bce loss in a single class)


\subsection{Optimizer}
Adam with lr=1e-3


\section{Validation}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.55]{docs/Project Report/Media/mlp_pca_accuracy_analysis.png}
    \caption{Effect of dimensionality of PCA-reduced dataset on MLP performance}
    \label{fig:my_label}
\end{figure}


\section{Interesting experiments}
\subsection{Train on 2 boxes, test on remaining box}
Test for accuracy of individual object categories


\subsection{Randomly sample dataset to get N samples and train MLP}
Test for influence of sample size on MLP accuracy


%----------------------------------------------------------------------------------
% Section 6: Future Work
%----------------------------------------------------------------------------------
\chapter{Future Work}
\label{chap:6}
things to mention
\begin{itemize}
    \item Since Pybullet is no longer being maintained, consider to move to mujico
    \item using real data may be better, because it is proven that tactile data collected from pybullet may not be accuracte enough
    \item use better object representations
    \item more analysis on different NNs
\end{itemize}




%----------------------------------------------------------------------------------
% Appendix section
%----------------------------------------------------------------------------------
\appendix
\printbibliography

\chapter{Other appendices, e.g., code listing}
Put your appendix sections here

\end{document}