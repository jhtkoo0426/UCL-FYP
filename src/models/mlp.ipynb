{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCL COMP0029 Individual Project for Year 3 BSc\n",
    "### Robust Robotic Grasping Utilising Touch Sensing - Proposed Learning Framework Notebook\n",
    "This notebook contains the essential code for training and testing a simplified learning framework for the proposed method - a simple multilayer perceptron to predict grasp outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device for `PyTorch` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty PyTorch cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load datasets from saved .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for this experiment, you can run the \"Collect Sensory Data\" button in the Pybullet simulation. This generates a predefined number of Gaussian grasps randomly generated from a base hand pose. Each individual grasp is considered as an individual experiment, and the data collected from this experiment is split into four, each stored in its own dataset.\n",
    "\n",
    "For all object models used in this experiment, each object has 4 datasets which include:\n",
    "- `depth_ds.npy` which stores the depth tactile data from the mounted DIGIT sensors\n",
    "- `color_ds.npy` which stores the colored (RGB) version of the depth tactile data from the mounted DIGIT sensors\n",
    "- `poses_ds.npy` which stores the randomly-generated 6d hand poses from the simulation\n",
    "- `outcomes_ds.npy` which stores the outcomes of each random pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../datasets/\"\n",
    "\n",
    "object_names = [\"block1\", \"block2\", \"block3\", \"cylinder1\", \"cylinder2\", \"cylinder3\"]\n",
    "\n",
    "depth_data = np.empty((0, 2, 160, 120))\n",
    "color_data = np.empty((0, 2, 160, 120, 3))\n",
    "poses_data = np.empty((0, 6))\n",
    "grasp_outcomes_data = np.empty((0,))\n",
    "\n",
    "for object_name in object_names:\n",
    "    # Construct the relative paths of each dataset and load them into the notebook\n",
    "    depth_ds_file_path = root + object_name + \"_ds/depth_ds.npy\"\n",
    "    depth_data1 = np.load(depth_ds_file_path)\n",
    "\n",
    "    color_ds_file_path = root + object_name + \"_ds/color_ds.npy\"\n",
    "    color_data1 = np.load(color_ds_file_path)\n",
    "\n",
    "    poses_ds_file_path = root + object_name + \"_ds/poses_ds.npy\"\n",
    "    poses_data1 = np.load(poses_ds_file_path)\n",
    "\n",
    "    grasp_outcomes_ds_file_path = root + object_name + \"_ds/grasp_outcomes.npy\"\n",
    "    grasp_outcomes_data1 = np.load(grasp_outcomes_ds_file_path)\n",
    "\n",
    "    depth_data = np.append(depth_data, depth_data1, axis=0)\n",
    "    color_data = np.append(color_data, color_data1, axis=0)\n",
    "    poses_data = np.append(poses_data, poses_data1, axis=0)\n",
    "    grasp_outcomes_data = np.append(grasp_outcomes_data, grasp_outcomes_data1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets should all be in the form of $(N\\times...)$ where $N$ is the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we confirm the number of successful and unsuccessful grasps recorded. This helps us in the next section to determine how many examples we should include for each class in order to produce a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Sampling\n",
    "We sample a pre-defined number of examples from each class label (successful and unsuccessful grasps) to reduce the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample n samples from the datasets\n",
    "def sample_data(depth_data, color_data, poses_data, outcomes_data, no_of_examples):\n",
    "    d, c, p, o = [], [], [], []\n",
    "    for label in np.unique(outcomes_data):\n",
    "        indices = np.where(outcomes_data == label)[0]\n",
    "\n",
    "        for i in range(no_of_examples):\n",
    "            d.append(depth_data[indices[i]])\n",
    "            c.append(color_data[indices[i]])\n",
    "            p.append(poses_data[indices[i]])\n",
    "            o.append(outcomes_data[indices[i]])\n",
    "    \n",
    "    depth_data = torch.from_numpy(np.array(d))\n",
    "    color_data = torch.from_numpy(np.array(c))\n",
    "    poses_data = torch.from_numpy(np.array(p))\n",
    "    outcomes_data = torch.from_numpy(np.array(o))\n",
    "    return depth_data, color_data, poses_data, outcomes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample `sample_size` samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data, color_data, poses_data, grasp_outcomes_data = sample_data(depth_data, color_data, poses_data, grasp_outcomes_data, no_of_examples=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")\n",
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    # Normalize & standardize each column\n",
    "    mean = torch.mean(tensor, axis=0)\n",
    "    std = torch.std(tensor, axis=0)\n",
    "    tensor = (tensor - mean) / std\n",
    "    tensor[torch.isnan(tensor)] = 0\n",
    "    tensor[torch.isinf(tensor)] = 0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def downsample_2d_tensor(tensor, factor=2):\n",
    "    return torch.nn.functional.avg_pool2d(tensor.unsqueeze(0), kernel_size=factor, stride=factor, padding=0).squeeze(0)\n",
    "\n",
    "\n",
    "def downsample_3d_tensor(tensor, factor=2):\n",
    "    pooling_kernel = 2\n",
    "    pooling_layer = torch.nn.AvgPool2d(kernel_size=pooling_kernel, stride=pooling_kernel)\n",
    "    downsampled_tensor = pooling_layer(tensor.permute(0, 3, 1, 2))\n",
    "    downsampled_tensor = downsampled_tensor.permute(0, 2, 3, 1)\n",
    "    \n",
    "    return downsampled_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Preparing datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a tactile+visual data representation with CNNs as the input for our MLP approach. We concatenate the depth and color to get our tactile dataset, then concatenate the tactile and visual datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = torch.cat([depth_data[:, 0, :, :], depth_data[:, 1, :, :]], dim=2)\n",
    "color_data = torch.cat([color_data[:, 0, :, :, :], color_data[:, 1, :, :, :]], dim=2)\n",
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "\n",
    "depth_ds = normalize(depth_data)\n",
    "color_ds = normalize(color_data)\n",
    "visual_ds = torch.from_numpy(np.nan_to_num(normalize(poses_data)))\n",
    "visual_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tactile_ds = torch.cat([depth_ds, color_ds], dim=1)\n",
    "# complete_ds = torch.cat([tactile_ds, visual_ds], dim=1)\n",
    "tactile_ds = torch.cat([depth_ds.unsqueeze(-1), color_ds], dim=-1)\n",
    "tactile_ds = torch.nan_to_num(tactile_ds)\n",
    "complete_ds = torch.cat([tactile_ds.reshape(tactile_ds.shape[0], -1), visual_ds], dim=1)\n",
    "complete_ds = torch.nan_to_num(complete_ds)\n",
    "print(tactile_ds.shape, complete_ds.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3d. CNN dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional neural network that extracts features from an input tensor\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data using CNN feature extraction\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn_tactile = torch.cat([cnn(img.float().permute(2,0,1)).unsqueeze(0) for img in tactile_ds])\n",
    "cnn_tactile = cnn_tactile.reshape(cnn_tactile.shape[0], -1)\n",
    "cnn_tactile.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing different data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our MLP model, there are two fully connected (dense) layers, each with an activation function (ReLU for the first layer and no activation for the second layer). The input size, hidden size, and output size are parameters that need to be specified when creating an instance of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def train_mlp(self, epochs, X_train, y_train):\n",
    "        self.losses = []\n",
    "        for _ in range(epochs):\n",
    "            inputs = torch.from_numpy(X_train).float()\n",
    "            labels = y_train.float().view(-1, 1)\n",
    "\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = self(inputs)\n",
    "            loss = self.criterion(preds, labels)\n",
    "            self.losses.append(loss.item())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "    def eval_mlp(self, X_test, y_test):\n",
    "        # Evaluate the performance of the model on the testing set\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(X_test).float()\n",
    "            labels = y_test.float().view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            final_preds = self(inputs)\n",
    "            predicted = (final_preds > 0).float()\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_test, predicted)\n",
    "            sns.heatmap(cm, linewidths=1, annot=True, fmt='g')\n",
    "\n",
    "            # Accuracy\n",
    "            accuracy = (predicted == labels).float().mean()\n",
    "            print(f\"Accuracy of MLP model: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.show\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Tactile only + CNN -> MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cnn_tactile.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp4a = MLP(input_size=512, hidden_size=64, output_size=1)\n",
    "mlp4a.train_mlp(epochs=1000, X_train=X_train, y_train=y_train)\n",
    "mlp4a.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp4a.eval_mlp(X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Visual only + CNN -> MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(visual_ds.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp4b = MLP(input_size=6, hidden_size=4, output_size=1)\n",
    "mlp4b.train_mlp(epochs=1000, X_train=X_train, y_train=y_train)\n",
    "mlp4b.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp4b.eval_mlp(X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Proposed method - All data -> MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training and testing datasets for future sections.\n",
    "We will only use the `complete_ds` for this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply combine the cnn-processed tactile data (from Section 5.3.1) with the visual data\n",
    "cnn5_complete_ds = torch.cat([cnn_tactile.reshape(cnn_tactile.shape[0], -1), visual_ds], dim=1)\n",
    "cnn5_complete_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn_complete_train, X_cnn_complete_test, y_cnn_complete_train, y_cnn_complete_test = train_test_split(cnn5_complete_ds.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp5 = MLP(input_size=518, hidden_size=64, output_size=1)\n",
    "mlp5.train_mlp(epochs=1000, X_train=X_cnn_complete_train, y_train=y_cnn_complete_train)\n",
    "mlp5.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp5.eval_mlp(X_test=X_cnn_complete_test, y_test=y_cnn_complete_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1153b3f7d4e5dcd3632df88b5826f13fc1401ea1edd930ba216b62830b02e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
