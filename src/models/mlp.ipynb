{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCL COMP0029 Individual Project for Year 3 BSc\n",
    "### Robust Robotic Grasping Utilising Touch Sensing - Proposed Learning Framework Notebook\n",
    "This notebook contains the essential code for training and testing a simplified learning framework for the proposed method - a simple Gaussian process generative model that infers good hand poses from existing data on a single grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device for `PyTorch` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty PyTorch cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load datasets from saved .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for this experiment, you can run the \"Collect Sensory Data\" button in the Pybullet simulation. This generates a predefined number of Gaussian grasps randomly generated from a base hand pose. Each individual grasp is considered as an individual experiment, and the data collected from this experiment is split into four, each stored in its own dataset.\n",
    "\n",
    "For all object models used in this experiment, each object has 4 datasets which include:\n",
    "- `depth_ds.npy` which stores the depth tactile data from the mounted DIGIT sensors\n",
    "- `color_ds.npy` which stores the colored (RGB) version of the depth tactile data from the mounted DIGIT sensors\n",
    "- `poses_ds.npy` which stores the randomly-generated 6d hand poses from the simulation\n",
    "- `outcomes_ds.npy` which stores the outcomes of each random pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../datasets/\"\n",
    "object_name = \"block\"      # object_name should be in [bottle, block, mug]\n",
    "\n",
    "# Construct the relative paths of each dataset and load them into the notebook\n",
    "depth_ds_file_path = root + object_name + \"/depth_ds.npy\"\n",
    "depth_data = np.load(depth_ds_file_path)\n",
    "\n",
    "color_ds_file_path = root + object_name + \"/color_ds.npy\"\n",
    "color_data = np.load(color_ds_file_path)\n",
    "\n",
    "poses_ds_file_path = root + object_name + \"/poses_ds.npy\"\n",
    "poses_data = np.load(poses_ds_file_path)\n",
    "\n",
    "grasp_outcomes_ds_file_path = root + object_name + \"/grasp_outcomes.npy\"\n",
    "grasp_outcomes_data = np.load(grasp_outcomes_ds_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets should all be in the form of $(N\\times...)$ where $N$ is the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of depth_data: (400, 2, 160, 120)\n",
      "Shape of color_data: (400, 2, 160, 120, 3)\n",
      "Shape of poses_data: (400, 6)\n",
      "Shape of grasp_outcomes_data: (400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we confirm the number of successful and unsuccessful grasps recorded. This helps us in the next section to determine how many examples we should include for each class in order to produce a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sucessesful grasps: 200\n",
      "# of unsuccessful grasps: 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Sampling\n",
    "We sample a pre-defined number of examples from each class label (successful and unsuccessful grasps) to reduce the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample n samples from the datasets\n",
    "def sample_data(depth_data, color_data, poses_data, outcomes_data, no_of_examples):\n",
    "    d, c, p, o = [], [], [], []\n",
    "    for label in np.unique(outcomes_data):\n",
    "        indices = np.where(outcomes_data == label)[0]\n",
    "\n",
    "        for i in range(no_of_examples):\n",
    "           # if np.mean(depth_data[indices[i]]) > 1e-7 and np.mean(color_data[indices[i]]) > 1e-7:\n",
    "            d.append(depth_data[indices[i]])\n",
    "            c.append(color_data[indices[i]])\n",
    "            p.append(poses_data[indices[i]])\n",
    "            o.append(outcomes_data[indices[i]])\n",
    "    \n",
    "    depth_data = torch.from_numpy(np.array(d))\n",
    "    color_data = torch.from_numpy(np.array(c))\n",
    "    poses_data = torch.from_numpy(np.array(p))\n",
    "    outcomes_data = torch.from_numpy(np.array(o))\n",
    "    return depth_data, color_data, poses_data, outcomes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample `sample_size` samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data, color_data, poses_data, grasp_outcomes_data = sample_data(depth_data, color_data, poses_data, grasp_outcomes_data, no_of_examples=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of depth_data: torch.Size([200, 2, 160, 120])\n",
      "Shape of color_data: torch.Size([200, 2, 160, 120, 3])\n",
      "Shape of poses_data: torch.Size([200, 6])\n",
      "Shape of grasp_outcomes_data: torch.Size([200])\n",
      "# of sucessesful grasps: 100\n",
      "# of unsuccessful grasps: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")\n",
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    # Normalize & standardize each column\n",
    "    mean = torch.mean(tensor, axis=0)\n",
    "    std = torch.std(tensor, axis=0)\n",
    "    tensor = (tensor - mean) / std\n",
    "    tensor[torch.isnan(tensor)] = 0\n",
    "    tensor[torch.isinf(tensor)] = 0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def downsample_2d_tensor(tensor, factor=2):\n",
    "    return torch.nn.functional.avg_pool2d(tensor.unsqueeze(0), kernel_size=factor, stride=factor, padding=0).squeeze(0)\n",
    "\n",
    "\n",
    "def downsample_3d_tensor(tensor, factor=2):\n",
    "    pooling_kernel = 2\n",
    "    pooling_layer = torch.nn.AvgPool2d(kernel_size=pooling_kernel, stride=pooling_kernel)\n",
    "    downsampled_tensor = pooling_layer(tensor.permute(0, 3, 1, 2))\n",
    "    downsampled_tensor = downsampled_tensor.permute(0, 2, 3, 1)\n",
    "    \n",
    "    return downsampled_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a tactile+visual data representation with CNNs as the input for our MLP approach. We concatenate the depth and color to get our tactile dataset, then concatenate the tactile and visual datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of depth_data: torch.Size([200, 160, 240])\n",
      "Shape of color_data: torch.Size([200, 160, 240, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2796, -0.9228,  0.1077,  0.2364, -0.0363,  0.2234],\n",
       "        [-0.1977,  1.8543,  3.1275,  0.7064, -0.7650, -0.8845],\n",
       "        [ 1.1225,  1.7721,  1.6135,  0.9806, -1.3460,  0.4886],\n",
       "        ...,\n",
       "        [-0.2629,  0.9141, -1.0400,  0.2867,  1.0805,  0.8738],\n",
       "        [ 0.3170, -0.0289, -2.1019, -0.8083,  1.0768, -1.7350],\n",
       "        [-0.8490,  0.5594, -1.1202,  1.2293,  0.6260, -0.9488]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_data = torch.cat([depth_data[:, 0, :, :], depth_data[:, 1, :, :]], dim=2)\n",
    "color_data = torch.cat([color_data[:, 0, :, :, :], color_data[:, 1, :, :, :]], dim=2)\n",
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "\n",
    "depth_ds = normalize(depth_data)\n",
    "color_ds = normalize(color_data)\n",
    "visual_ds = torch.from_numpy(np.nan_to_num(normalize(poses_data)))\n",
    "visual_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 153606])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tactile_ds = torch.cat([depth_ds, color_ds], dim=1)\n",
    "# complete_ds = torch.cat([tactile_ds, visual_ds], dim=1)\n",
    "tactile_ds = torch.cat([depth_ds.unsqueeze(-1), color_ds], dim=-1)\n",
    "tactile_ds = torch.nan_to_num(tactile_ds)\n",
    "complete_ds = torch.cat([tactile_ds.reshape(tactile_ds.shape[0], -1), visual_ds], dim=1)\n",
    "complete_ds = torch.nan_to_num(complete_ds)\n",
    "complete_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use the `complete_ds` for this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. CNN dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional neural network that extracts features from an input tensor\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 518])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data using CNN feature extraction\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn_tactile = torch.cat([cnn(img.float().permute(2,0,1)).unsqueeze(0) for img in tactile_ds])\n",
    "cnn_tactile = cnn_tactile.reshape(cnn_tactile.shape[0], -1)\n",
    "cnn_tactile.shape\n",
    "\n",
    "# We simply combine the cnn-processed tactile data (from Section 5.3.1) with the visual data\n",
    "cnn_complete_ds = torch.cat([cnn_tactile.reshape(cnn_tactile.shape[0], -1), visual_ds], dim=1)\n",
    "cnn_complete_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training and testing datasets for future sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn_complete_train, X_cnn_complete_test, y_cnn_complete_train, y_cnn_complete_test = train_test_split(cnn_complete_ds.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our MLP model, there are two fully connected (dense) layers, each with an activation function (ReLU for the first layer and no activation for the second layer). The input size, hidden size, and output size are parameters that need to be specified when creating an instance of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of our MLP with the desired input size and output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP model: 90.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "mlp = MLP(input_size=518, hidden_size=64, output_size=1)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(mlp.parameters())\n",
    "\n",
    "# Train the MLP on the training set\n",
    "for epoch in range(100):\n",
    "    inputs = torch.from_numpy(X_cnn_complete_train).float()\n",
    "    labels = y_cnn_complete_train.float().view(-1, 1)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    preds = mlp(inputs)\n",
    "    loss = criterion(preds, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the performance of the model on the testing set\n",
    "with torch.no_grad():\n",
    "    inputs = torch.from_numpy(X_cnn_complete_test).float()\n",
    "    labels = y_cnn_complete_test.float().view(-1, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    final_preds = mlp(inputs)\n",
    "    predicted = (final_preds > 0).float()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_cnn_complete_test, predicted)\n",
    "    sns.heatmap(cm, linewidths=1, annot=True, fmt='g')\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (predicted == labels).float().mean()\n",
    "    print(f\"Accuracy of MLP model: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1153b3f7d4e5dcd3632df88b5826f13fc1401ea1edd930ba216b62830b02e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
