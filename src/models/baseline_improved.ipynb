{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCL COMP0029 Individual Project for Year 3 BSc\n",
    "### Robust Robotic Grasping Utilising Touch Sensing - Baseline Approach Notebook\n",
    "This notebook contains the code for developing a baseline approach to grasping using classifiers: given some combinations of tactile data, end effector poses relative to the robot hand (visual data), etc., determine whether these constraints will produce a successful/unsuccessful grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device for `PyTorch` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty PyTorch cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set numpy seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load datasets from saved .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for this experiment, you can run the \"Collect Sensory Data\" button in the Pybullet simulation. This generates a predefined number of Gaussian grasps randomly generated from a base hand pose. Each individual grasp is considered as an individual experiment, and the data collected from this experiment is split into four, each stored in its own dataset.\n",
    "\n",
    "For all object models used in this experiment, each object has 4 datasets which include:\n",
    "- `depth_ds.npy` which stores the depth tactile data from the mounted DIGIT sensors\n",
    "- `color_ds.npy` which stores the colored (RGB) version of the depth tactile data from the mounted DIGIT sensors\n",
    "- `poses_ds.npy` which stores the randomly-generated 6d hand poses from the simulation\n",
    "- `outcomes_ds.npy` which stores the outcomes of each random pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../datasets/\"\n",
    "object_name = \"block\"      # object_name should be in [bottle, block, mug]\n",
    "\n",
    "# Construct the relative paths of each dataset and load them into the notebook\n",
    "depth_ds_file_path = root + object_name + \"/depth_ds.npy\"\n",
    "depth_data = np.load(depth_ds_file_path)\n",
    "\n",
    "color_ds_file_path = root + object_name + \"/color_ds.npy\"\n",
    "color_data = np.load(color_ds_file_path)\n",
    "\n",
    "poses_ds_file_path = root + object_name + \"/poses_ds.npy\"\n",
    "poses_data = np.load(poses_ds_file_path)\n",
    "\n",
    "grasp_outcomes_ds_file_path = root + object_name + \"/grasp_outcomes.npy\"\n",
    "grasp_outcomes_data = np.load(grasp_outcomes_ds_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets should all be in the form of $(N\\times...)$ where $N$ is the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we confirm the number of successful and unsuccessful grasps recorded. This helps us in the next section to determine how many examples we should include for each class in order to produce a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a logistic regression classifier on 3 combinations of data:\n",
    "- **Tactile** only: concatenated and flattened depth and color data\n",
    "- **Visual** only: 6D end effector poses consisting of position (x,y,z) and orientation (r,p,y) data\n",
    "- **Both**: concatenated and flattened tactile and visual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Sampling\n",
    "We sample a pre-defined number of examples from each class label (successful and unsuccessful grasps) to reduce the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample n samples from the datasets\n",
    "def sample_data(depth_data, color_data, poses_data, outcomes_data, no_of_examples):\n",
    "    d, c, p, o = [], [], [], []\n",
    "    for label in np.unique(outcomes_data):\n",
    "        indices = np.where(outcomes_data == label)[0]\n",
    "\n",
    "        for i in range(no_of_examples):\n",
    "           # if np.mean(depth_data[indices[i]]) > 1e-7 and np.mean(color_data[indices[i]]) > 1e-7:\n",
    "            d.append(depth_data[indices[i]])\n",
    "            c.append(color_data[indices[i]])\n",
    "            p.append(poses_data[indices[i]])\n",
    "            o.append(outcomes_data[indices[i]])\n",
    "    \n",
    "    depth_data = torch.from_numpy(np.array(d))\n",
    "    color_data = torch.from_numpy(np.array(c))\n",
    "    poses_data = torch.from_numpy(np.array(p))\n",
    "    outcomes_data = torch.from_numpy(np.array(o))\n",
    "    return depth_data, color_data, poses_data, outcomes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample `sample_size` samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data, color_data, poses_data, grasp_outcomes_data = sample_data(depth_data, color_data, poses_data, grasp_outcomes_data, no_of_examples=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")\n",
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some reusable functions for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    # Normalize & standardize each column\n",
    "    mean = torch.mean(tensor, axis=0)\n",
    "    std = torch.std(tensor, axis=0)\n",
    "    tensor = (tensor - mean) / std\n",
    "    tensor[torch.isnan(tensor)] = 0\n",
    "    tensor[torch.isinf(tensor)] = 0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def downsample_2d_tensor(tensor, factor=2):\n",
    "    return torch.nn.functional.avg_pool2d(tensor.unsqueeze(0), kernel_size=factor, stride=factor, padding=0).squeeze(0)\n",
    "\n",
    "\n",
    "def downsample_3d_tensor(tensor, factor=2):\n",
    "    pooling_kernel = 2\n",
    "    pooling_layer = torch.nn.AvgPool2d(kernel_size=pooling_kernel, stride=pooling_kernel)\n",
    "    downsampled_tensor = pooling_layer(tensor.permute(0, 3, 1, 2))\n",
    "    downsampled_tensor = downsampled_tensor.permute(0, 2, 3, 1)\n",
    "    \n",
    "    return downsampled_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each tactile reading (depth and color) is a pair of images (one on each finger), we concatenate them together as a single 160x240 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = torch.cat([depth_data[:, 0, :, :], depth_data[:, 1, :, :]], dim=2)\n",
    "color_data = torch.cat([color_data[:, 0, :, :, :], color_data[:, 1, :, :, :]], dim=2)\n",
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate the depth and color datasets to produce the flattened tactile dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_ds = depth_data.reshape(depth_data.shape[0], -1)\n",
    "# color_ds = color_data.reshape(color_data.shape[0], -1)\n",
    "# depth_ds.shape,  color_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Normalising the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_ds = normalize(depth_ds)\n",
    "# color_ds = normalize(color_ds)\n",
    "depth_ds = normalize(depth_data)\n",
    "color_ds = normalize(color_data)\n",
    "visual_ds = torch.from_numpy(np.nan_to_num(normalize(poses_data)))\n",
    "visual_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tactile dataset and the complete dataset (tactile+visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tactile_ds = torch.cat([depth_ds, color_ds], dim=1)\n",
    "# complete_ds = torch.cat([tactile_ds, visual_ds], dim=1)\n",
    "tactile_ds = torch.cat([depth_ds.unsqueeze(-1), color_ds], dim=-1)\n",
    "tactile_ds = torch.nan_to_num(tactile_ds)\n",
    "complete_ds = torch.cat([tactile_ds.reshape(tactile_ds.shape[0], -1), visual_ds], dim=1)\n",
    "complete_ds = torch.nan_to_num(complete_ds)\n",
    "tactile_ds.shape, complete_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset visualisation (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this section to produce figures and plots for the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = np.random.choice(len(depth_data), size=10)\n",
    "cols = 4\n",
    "rows = 5\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "\n",
    "# Note that the plots use the \"..._data\" datasets instead of the \"..._ds\" datasets since \n",
    "# the \"..._ds\" datasets are already flattened for training\n",
    "for i in range(rows*cols):\n",
    "    fig.add_subplot(rows, cols, i+1)\n",
    "    if i % 2 == 0:\n",
    "        plt.xlabel(\"Depth\")\n",
    "        plt.ylabel(\"SUCCESS\" if grasp_outcomes_data[rand_indices[i//2]].item() == 1.0 else \"FAILURE\")\n",
    "        plt.imshow(np.array(depth_data[rand_indices[i//2]]), cmap='gray')\n",
    "    elif i % 2 == 1:\n",
    "        plt.xlabel(\"Color\")\n",
    "        plt.imshow(np.array(color_data[rand_indices[i//2]]) / 255., cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the end effector poses to investigate if we can simply visualise a clear decision boundary between successful & unsuccessful grasps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to calculate approach vectors of a 6D pose\n",
    "def calc_approach_vector(coord):\n",
    "    R = np.array([[np.cos(coord[5])*np.cos(coord[4]), np.cos(coord[5])*np.sin(coord[4])*np.sin(coord[3])-np.sin(coord[5])*np.cos(coord[3]), np.cos(coord[5])*np.sin(coord[4])*np.cos(coord[3])+np.sin(coord[5])*np.sin(coord[3])],\n",
    "              [np.sin(coord[5])*np.cos(coord[4]), np.sin(coord[5])*np.sin(coord[4])*np.sin(coord[3])+np.cos(coord[5])*np.cos(coord[3]), np.sin(coord[5])*np.sin(coord[4])*np.cos(coord[3])-np.cos(coord[5])*np.sin(coord[3])],\n",
    "              [-np.sin(coord[4]), np.cos(coord[4])*np.sin(coord[3]), np.cos(coord[4])*np.cos(coord[3])]])\n",
    "    return R.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "\n",
    "\n",
    "Z_PADDING = 0.2        # Adjust for visualisation of end poses\n",
    "\n",
    "plot = plt.figure()\n",
    "ax = plot.add_subplot(projection='3d', computed_zorder=False)\n",
    "ax.axes.set_xlim3d(-0.05, 0.05)\n",
    "ax.axes.set_ylim3d(-0.05, 0.05)\n",
    "ax.axes.set_zlim3d(0, 0.1)\n",
    "\n",
    "    \n",
    "# Dimensions of the block object: (W=0.025, H=0.05, D=0.05)\n",
    "# (0,0,0), (0,1,0), (1,0,0), (0,0,1) -> unit cube\n",
    "cube_definition = [(-0.0125, -0.025, 0), (0.0125, -0.025, 0), (-0.0125, 0.0255, 0), (-0.0125, -0.025, 0.05)]\n",
    "plot_cube(cube_definition, ax)\n",
    "# Plot hand poses (cartesian)\n",
    "# ax.scatter(poses_data[:, 0], poses_data[:, 1], poses_data[:, 2], color=['red' if g==1 else 'green' for g in grasp_outcomes_data])\n",
    "for i in range(poses_data.shape[0]):\n",
    "    x, y, z, alpha, beta, gamma = poses_data[i, :]\n",
    "    z -= Z_PADDING * 1.65\n",
    "\n",
    "    # create rotation matrix based on Euler angles\n",
    "    Rx = np.array([[1, 0, 0], [0, cos(alpha), -sin(alpha)], [0, sin(alpha), cos(alpha)]])\n",
    "    Ry = np.array([[cos(beta), 0, sin(beta)], [0, 1, 0], [-sin(beta), 0, cos(beta)]])\n",
    "    Rz = np.array([[cos(gamma), -sin(gamma), 0], [sin(gamma), cos(gamma), 0], [0, 0, 1]])\n",
    "    R = Rz.dot(Ry.dot(Rx))\n",
    "\n",
    "    # calculate endpoints of line based on orientation\n",
    "    vec = np.array([0, 0.015, 0])\n",
    "    vec_rotated1 = R.dot(vec)\n",
    "    endpoint1 = [x + vec_rotated1[0], y + vec_rotated1[1], z + vec_rotated1[2]]\n",
    "    \n",
    "    vec_rotated2 = R.dot(-vec)\n",
    "    endpoint2 = [x + vec_rotated2[0], y + vec_rotated2[1], z + vec_rotated2[2]]\n",
    "\n",
    "    # set midpoint to the actual point\n",
    "    midpoint = [x, y, z]\n",
    "\n",
    "    # plot line through point in orientation direction\n",
    "    ax.plot([endpoint1[0], endpoint2[0]], [endpoint1[1], endpoint2[1]], [endpoint1[2], endpoint2[2]], color='red' if grasp_outcomes_data[i]==1 else 'green', zorder=20)\n",
    "\n",
    "    # plot vertical lines starting from endpoints\n",
    "    ax.plot([endpoint1[0], endpoint1[0]], [endpoint1[1], endpoint1[1]], [endpoint1[2], endpoint1[2]-0.025], color='red' if grasp_outcomes_data[i]==1 else 'green', zorder=20)\n",
    "    ax.plot([endpoint2[0], endpoint2[0]], [endpoint2[1], endpoint2[1]], [endpoint2[2], endpoint2[2]-0.025], color='red' if grasp_outcomes_data[i]==1 else 'green', zorder=20)\n",
    "\n",
    "\n",
    "# Draw block object\n",
    "def plot_cube(cube_definition, ax):\n",
    "    cube_definition_array = [np.array(list(item)) for item in cube_definition]\n",
    "    points = []\n",
    "    points += cube_definition_array\n",
    "    vectors = [\n",
    "        cube_definition_array[1] - cube_definition_array[0],\n",
    "        cube_definition_array[2] - cube_definition_array[0],\n",
    "        cube_definition_array[3] - cube_definition_array[0]\n",
    "    ]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[1] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1] + vectors[2]]\n",
    "    points = np.array(points)\n",
    "    edges = [\n",
    "        [points[0], points[3], points[5], points[1]],\n",
    "        [points[1], points[5], points[7], points[4]],\n",
    "        [points[4], points[2], points[6], points[7]],\n",
    "        [points[2], points[6], points[3], points[0]],\n",
    "        [points[0], points[2], points[4], points[1]],\n",
    "        [points[3], points[6], points[7], points[5]]\n",
    "    ]\n",
    "    faces = Poly3DCollection(edges, linewidths=1, edgecolors='k')\n",
    "    faces.set_facecolor((0,0,1,0.1))\n",
    "    faces.set_zorder(3)\n",
    "    ax.add_collection3d(faces)\n",
    "    # Plot the points themselves to force the scaling of the axes\n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], s=0)\n",
    "    # ax.set_aspect('equal')\n",
    "\n",
    "# Display the plot at different angles\n",
    "ax.view_init(elev=15, azim=30, roll=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the green and red points are mixed up together, it proves that there is a problem when generating the random poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our Logistic Regression models on the 3 combinations of our data (tactile, visual, both):\n",
    "- Raw data\n",
    "- Principal Component Analysis - 2 main components\n",
    "- Convolutional Neural Network processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tactile_train, X_tactile_test, y_tactile_train, y_tactile_test = train_test_split(tactile_ds.reshape(tactile_ds.shape[0], -1), grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_visual_train, X_visual_test, y_visual_train, y_visual_test = train_test_split(visual_ds, grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_complete_train, X_complete_test, y_complete_train, y_complete_test = train_test_split(complete_ds, grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Raw data (tactile only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_511 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_511.fit(X_tactile_train, y_tactile_train)\n",
    "model_511_predictions = model_511.predict(X_tactile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Raw data (visual only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_512 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_512.fit(X_visual_train, y_visual_train)\n",
    "model_512_predictions = model_512.predict(X_visual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 Raw data (both) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_513 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_513.fit(X_complete_train, y_complete_train)\n",
    "model_513_predictions = model_513.predict(X_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may not be enough memory to perform the PCA calculation on the entire input tensor. One solution to this issue is to perform PCA in a mini-batch fashion, where you break down the input tensor into smaller chunks and perform PCA on each chunk separately. Then, you can concatenate the results from each chunk to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_multid(tensor, k):\n",
    "    tensor = tensor.reshape((tensor.shape[0], -1))\n",
    "    pca = PCA(n_components=k)    \n",
    "    pca.fit(tensor)\n",
    "    return pca.transform(tensor)\n",
    "\n",
    "def pca_2d(tensor, k):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(tensor)\n",
    "    return pca.transform(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "pca_tactile_ds = torch.Tensor(np.array([pca_multid(i, k=3) for i in tactile_ds]))\n",
    "pca_visual_ds = torch.Tensor(pca_2d(visual_ds, k=K))\n",
    "pca_complete_ds = torch.Tensor(pca_2d(complete_ds, k=K))\n",
    "pca_tactile_ds.shape, pca_visual_ds.shape, pca_complete_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_tactile_train, X_pca_tactile_test, y_pca_tactile_train, y_pca_tactile_test = train_test_split(pca_tactile_ds.reshape(pca_tactile_ds.shape[0], -1), grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_pca_visual_train, X_pca_visual_test, y_pca_visual_train, y_pca_visual_test = train_test_split(pca_visual_ds, grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_pca_complete_train, X_pca_complete_test, y_pca_complete_train, y_pca_complete_test = train_test_split(pca_complete_ds, grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 PCA (tactile only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_521 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_521.fit(X_pca_tactile_train, y_pca_tactile_train)\n",
    "model_521_predictions = model_521.predict(X_pca_tactile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 PCA (visual only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_522 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_522.fit(X_pca_visual_train, y_pca_visual_train)\n",
    "model_522_predictions = model_522.predict(X_pca_visual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 PCA (both) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_523 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_523.fit(X_pca_complete_train, y_pca_complete_train)\n",
    "model_523_predictions = model_523.predict(X_pca_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CNN for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional neural network that extracts features from an input tensor\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data using CNN feature extraction\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn_tactile = torch.cat([cnn(img.float().permute(2,0,1)).unsqueeze(0) for img in tactile_ds])\n",
    "cnn_tactile = cnn_tactile.reshape(cnn_tactile.shape[0], -1)\n",
    "cnn_tactile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply combine the cnn-processed tactile data (from Section 5.3.1) with the visual data\n",
    "cnn_complete_ds = torch.cat([cnn_tactile.reshape(cnn_tactile.shape[0], -1), visual_ds], dim=1)\n",
    "cnn_complete_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn_tactile_train, X_cnn_tactile_test, y_cnn_tactile_train, y_cnn_tactile_test = train_test_split(cnn_tactile.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_cnn_complete_train, X_cnn_complete_test, y_cnn_complete_train, y_cnn_complete_test = train_test_split(cnn_complete_ds.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 CNN (tactile only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_531 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_531.fit(X_cnn_tactile_train, y_cnn_tactile_train)\n",
    "model_531_predictions = model_531.predict(X_cnn_tactile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 CNN (visual only) + LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 CNN (both) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_533 = LogisticRegression(random_state=0, max_iter=400)\n",
    "model_533.fit(X_cnn_complete_train, y_cnn_complete_train)\n",
    "model_533_predictions = model_533.predict(X_cnn_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"Raw (tactile only)\": {\"score\": model_511.score(X_tactile_test, y_tactile_test), \"preds\": model_511_predictions, \"test_set\": y_tactile_test},\n",
    "    \"Raw (visual only)\": {\"score\": model_512.score(X_visual_test, y_visual_test), \"preds\": model_512_predictions, \"test_set\": y_visual_test},\n",
    "    \"Raw (both)\": {\"score\": model_513.score(X_complete_test, y_complete_test),\"preds\": model_513_predictions,\"test_set\": y_complete_test},\n",
    "    \"PCA (tactile only)\": {\"score\": model_521.score(X_pca_tactile_test, y_pca_tactile_test),\"preds\": model_521_predictions,\"test_set\": y_pca_tactile_test},\n",
    "    \"PCA (visual only)\": {\"score\": model_522.score(X_pca_visual_test, y_pca_visual_test),\"preds\": model_522_predictions,\"test_set\": y_pca_visual_test},\n",
    "    \"PCA (both)\": {\"score\": model_523.score(X_pca_complete_test, y_pca_complete_test),\"preds\": model_523_predictions,\"test_set\": y_pca_complete_test},\n",
    "    \"CNN (tactile only)\": {\"score\": model_531.score(X_cnn_tactile_test, y_cnn_tactile_test), \"preds\": model_531_predictions,\"test_set\": y_cnn_tactile_test},\n",
    "    \"CNN (both)\": {\"score\": model_533.score(X_cnn_complete_test, y_cnn_complete_test), \"preds\": model_533_predictions, \"test_set\":y_cnn_complete_test},\n",
    "}\n",
    "\n",
    "fig, axn = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(12,10))\n",
    "\n",
    "for i, ax in enumerate(axn.flat):\n",
    "    try:\n",
    "        model_name = list(model_data.keys())[i]\n",
    "        model = model_data[model_name]\n",
    "        preds = model[\"preds\"]\n",
    "        score = model[\"score\"]\n",
    "        test_set = model[\"test_set\"]\n",
    "\n",
    "        if score is not None and preds is not None and test_set is not None:\n",
    "            cm = confusion_matrix(test_set, preds)\n",
    "            sns.heatmap(cm, linewidths=1, ax=ax, annot=True, fmt='g')\n",
    "            ax.set_title(model_name + f\": {score*100:.2f}%\", fontsize=8)\n",
    "        else:\n",
    "            ax.set_title(model_name + \"No results\", fontsize=8)\n",
    "        continue\n",
    "    except IndexError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1153b3f7d4e5dcd3632df88b5826f13fc1401ea1edd930ba216b62830b02e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
